import streamlit as st
import feedparser
from langdetect import detect
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# ì–¸ì–´ë³„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from konlpy.tag import Okt
import jieba
from janome.tokenizer import Tokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

nltk.download("punkt")
nltk.download("stopwords")

# í•œêµ­ì–´/ì¼ë³¸ì–´ ê°ì²´
okt = Okt()
janome_tagger = Tokenizer()

# âœ… ë¶ˆìš©ì–´ ì‚¬ì „
stopwords_ko = {"ê²ƒ","ìˆ˜","ë“±","ë“¤","ë°","ì—ì„œ","í•˜ë‹¤","ê¹Œì§€","ë¶€í„°","ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","ë•Œë¬¸","ì´ê²ƒ","ì €ê²ƒ","ê·¸ê²ƒ"}
stopwords_ja = {"ã“ã¨","ã“ã‚Œ","ãã‚Œ","ãŸã‚","ã‚ˆã†","ã‚‚ã®","ã•ã‚“","ã—ã¦","ã„ã‚‹","ã‚ã‚‹","ãªã‚‹","ã¾ãŸ","ãã—ã¦","ã—ã‹ã—"}
stopwords_zh = {"çš„","äº†","åœ¨","æ˜¯","æˆ‘","æœ‰","å’Œ","å°±","ä¸","äºº","éƒ½","ä¸€ä¸ª","ä¸Š","ä¹Ÿ","å¾ˆ","åˆ°","è¯´","è¦","å»","ä½ "}
stopwords_en = set(stopwords.words("english")) | {"said","one","like","also","would","could","us","many","new","people"}

def build_query(include_terms=None, exclude_terms=None, mode="AND"):
    query_parts = []
    if include_terms:
        if mode.upper() == "AND":
            query_parts.append(" ".join(include_terms))
        elif mode.upper() == "OR":
            query_parts.append("(" + " OR ".join(include_terms) + ")")
    if exclude_terms:
        query_parts.append(" ".join(f"-{term.strip()}" for term in exclude_terms))
    return " ".join(query_parts)


def get_text_before_dash(text: str) -> str:
    if "-" in text:
        return text.split("-")[0]
    return text

# ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
def fetch_news(query, site=None, lang="en", region="US"):
    from urllib.parse import quote
    query = quote(query)
    print(f"â–¶ Fetching news for query: {query}, lang: {lang}, region: {region}")
    if site:
        query = f"{query} site:{site}"
    url = f"https://news.google.com/rss/search?q={query}&hl={lang}&gl={region}&ceid={region}:{lang}"
    feed = feedparser.parse(url)
    #return [entry.title + " " + entry.description for entry in feed.entries]
    return [get_text_before_dash(entry.title) for entry in feed.entries]

# í† í¬ë‚˜ì´ì¦ˆ
def tokenize_and_clean(text, lang):
    tokens = []
    if lang == "ko":
        tokens = [w for w, pos in okt.pos(text) if pos in ["Noun", "Adjective"]]
        tokens = [w for w in tokens if w not in stopwords_ko]
    elif lang == "ja":
        tokens = [t.surface for t in janome_tagger.tokenize(text) 
                  if t.part_of_speech.startswith("åè©") or t.part_of_speech.startswith("å½¢å®¹è©")]
        tokens = [w for w in tokens if w not in stopwords_ja]
    elif lang == "zh":
        tokens = [w for w in jieba.cut(text) if w not in stopwords_zh and len(w) > 1]
    else:  # ì˜ì–´ ê¸°ë³¸
        tokens = [w for w in word_tokenize(text) if w.isalpha() and w.lower() not in stopwords_en]
    return [w.lower().strip() for w in tokens if w.strip()]

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def run_pipeline(query, site=None, lang="en", region="US"):
    docs = fetch_news(query, site, lang, region)
    all_tokens = []
    for doc in docs:
        try:
            detected_lang = detect(doc)
        except:
            detected_lang = "en"
        all_tokens.extend(tokenize_and_clean(doc, detected_lang))
    return Counter(all_tokens), len(docs)

# ===============================
# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
# ===============================
st.title("ğŸŒ ë‹¤êµ­ì–´ ë‰´ìŠ¤ ì›Œë“œí´ë¼ìš°ë“œ")

query1 = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì½¤ë§ˆë¡œ ë¶„ë¦¬í•´ì„œ ì…ë ¥í•˜ì„¸ìš”", "ê¹€ì£¼ì• ").split(",")
query2 = st.text_input("ê²€ìƒ‰ ë°°ì œ ë‹¨ì–´ë¥¼ ì½¤ë§ˆë¡œ ë¶„ë¦¬í•´ì„œ ì…ë ¥í•˜ì„¸ìš”", "").split(",")

query = build_query(include_terms=query1, exclude_terms=query2, mode="AND")
print(f"â–¶ ìµœì¢… ê²€ìƒ‰ì–´: {query}")

col1, col2 = st.columns([1, 1])  

with col1:
    lang = st.selectbox("ì–¸ì–´", ["í•œêµ­ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´", "ì˜ì–´"])
    lang_option = {"í•œêµ­ì–´":"ko", "ì¼ë³¸ì–´":"ja", "ì¤‘êµ­ì–´":"zh-CN", "ì˜ì–´":"en"}[lang]
with col2:
    region = st.selectbox("ì§€ì—­", ["í•œêµ­", "ì¼ë³¸", "ì¤‘êµ­", "ë¯¸êµ­"])
    region_option = {"í•œêµ­":"KR", "ì¼ë³¸":"JP", "ì¤‘êµ­":"CN", "ë¯¸êµ­":"US"}[region]

if st.button("ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± Submit Here â–¶ "):
    freq, len_docs = run_pipeline(query, lang=lang_option, region=region_option)
    if not freq:
        st.warning("âš ï¸ ë‰´ìŠ¤ì—ì„œ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # âœ… ìš´ì˜ì²´ì œì— ë§ëŠ” í°íŠ¸ ê²½ë¡œ ì§€ì • (ìœˆë„ìš° ê¸°ë³¸ê°’)
        font_path = "C:/Windows/Fonts/malgun.ttf"

        wc = WordCloud(
            font_path=font_path,
            width=800, height=600, background_color="white"
        ).generate_from_frequencies(freq)

        fig, ax = plt.subplots(figsize=(10, 8))
        ax.imshow(wc, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)
        st.success(f"âœ… {len_docs}ê°œì˜ ë‰´ìŠ¤ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")